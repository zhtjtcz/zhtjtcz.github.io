<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haitao Zhou</title>
  
  <meta name="author" content="Haitao Zhou">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name><strong><font size="4px">Haitao Zhou (周海涛)</font></strong></name>
              </p>
              <p> I am a Master's student from Beihang University(BUAA).
				And I am also an algorithm intern of <a href="https://aishiai.com/">AIsphere</a>, 
				where I work on video generation.
              <p> I got my Undergraduate's degree from the BUAA.
              <p> Recently, my research focuses on <b>deep generative models(Diffusion models and its application) and sketch&SVG generation</b>. 
			 
              
			  <p style="text-align:center">
                <a href="mailto:marvolo@buaa.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com.hk/citations?user=e_kUVpcAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/zhtjtcz/">Github</a>
              </p>

            </td>
            
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="./photos/jiangyi.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="./photos/zht.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

		<!--         
	<p style="text-align:left"><strong><font size="4px">Research Highlight</font></strong></p> 
		<ul>
      <li><p style="text-align:left"><b>UNINEXT</b> accepted by CVPR'23. UNINEXT unifies 10 instance perception tasks using a single model with the same model parameters</p></li> 
		  
      <li><p style="text-align:left"><b>ByteTrack</b> ranks <a href="https://www.paperdigest.org/2023/01/most-influential-eccv-papers-2023-01/"; style="color: #EE7F2D;"> <b>1th of the most influential papers in ECCV 2022.</b></a> Code is available on github with 3.4k stars </p></li>
      
      <li><p style="text-align:left"><b>Spark</b> accepted by ICLR'23 as <b style="color: red">Spotlight. </b>Spark is the first successful BERT/MAE-style pretraining on any convolutional networks</p></li>
      
      <li><p style="text-align:left"><b>Unicorn</b> accepted by ECCV'22 as <b style="color: red">Oral Presentation</b> . Unicorn accomplishes the great unification of the tracking network architecture and learning paradigm</p></li>
      
      <li><p style="text-align:left"><b>IDOL</b> and <b>Seqformer</b> accepted by ECCV'22 as <b style="color: red">Oral Presentation</b> , serving as strong baseline for video instance segmentation</p></li> 
      
      <li><p style="text-align:left"><b>Sparse R-CNN</b> accepted by CVPR'21. Sparse R-CNN is integrated into several famous frameworks(Detectron2, MMDetection, PaddlePaddle)</p></li>
    	</ul> -->
	
	<br>

 
 	<p style="text-align:justify"><strong><font size="4px">Publications</font></strong> (* equal contribution, <sup>†</sup> corresponding author)
	<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">

    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/svgdreamer_cvpr2024.png" style="height: 160px; width: 320px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.16476">
              <papertitle><b>SVGDreamer: Text Guided SVG Generation with Diffusion Model</b></papertitle>
              </a>
              <br>
              Xingming Xing, <b>Haitao Zhou</b>, Chuang Wang, Jing Zhang, Dong Xu, Qian Yu<sup>†</sup>
              <br>
              <em>Computer Vision and Pattern Recognition(<b>CVPR</b>)</em>, 2024
              <br>

			  <a href="https://arxiv.org/abs/2312.16476"; style="color: #EE7F2D;">Paper</a>
			  /
			  <a href="https://github.com/ximinng/SVGDreamer"; style="color: #EE7F2D;">Code</a>
			  <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ximinng/SVGDreamer.svg" alt="GitHub stars" title="" />
            </td><tr> 


        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/diffsketcher_nips2023.png" style="height: 160px; width: 320px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2306.14685">
              <papertitle><b>DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models</b></papertitle>
              </a>
              <br>
              Xingming Xing, Chuang Wang, <b>Haitao Zhou</b>, Jing Zhang, Qian Yu<sup>†</sup>, Dong Xu
              <br>
              <em>Neural Information Processing Systems (<b>NeurIPS</b>)</em>, 2023
              <br>

			<a href="https://arxiv.org/abs/2306.14685"; style="color: #EE7F2D;">Paper</a>
			/
            <a href="https://github.com/ximinng/DiffSketcher"; style="color: #EE7F2D;">Code</a>
            <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ximinng/DiffSketcher.svg" alt="GitHub stars" title="" />
            </td><tr>
						       
</table></p>

<p style="text-align:justify"><strong><font size="4px">Preprints</font></strong> (* equal contribution, <sup>†</sup> corresponding author)
    <table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">
      
      
     <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/inversion_iccv2023.png" style="height: 160px; width: 320px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2308.07665">
              <papertitle><b>Inversion-by-Inversion: Exemplar-based Sketch-to-Photo Synthesis via Stochastic Differential Equations without Training</b></papertitle>
              </a>
              <br>
              Xingming Xing, Chuang Wang, <b>Haitao Zhou</b>, Zhihao Hu, Chongxuan Li, Dong Xu, Qian Yu<sup>†</sup>              
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2308.07665"; style="color: #EE7F2D;">arXiv</a>
              /
              <a href="https://github.com/ximinng/inversion-by-inversion"; style="color: #EE7F2D;">Code</a>
			  <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ximinng/inversion-by-inversion.svg" alt="GitHub stars" title="" /> 
			  <p></p>
            </td><tr> 

        </table></p>

		<p style="text-align:justify"><strong><font size="4px">Project</font></strong>
			<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">
			  
			  
			 <td style="padding:20px;width:25%;vertical-align:middle">
					<img src="./photos/svgrender.png" style="height: 160px; width: 320px; margin-top: 10px">
					<td style="padding:10px;width:75%;vertical-align:middle">
					  <a href="https://arxiv.org/abs/2308.07665">
					  <papertitle><b>PyTorch-SVGRender</b></papertitle>
					  </a>
					  <br>
					  <p>
						PyTorch-SVGRender is the go-to library for differentiable rendering methods in SVG generation. 
						It supports a variety of vectorization methods, including: <b>Image-to-SVG, Text-to-SVG, Text-to-Sketch.</b> 
					  </p> 
					  <em>Based on Pytorch</em>, 2023
					  <br>
					  <a href="https://qianyu-lab.github.io/PyTorch-SVGRender-project/"; style="color: #EE7F2D;">Project Page</a>
					  /
					  <a href="https://pytorch-svgrender.readthedocs.io/en/latest/index.html"; style="color: #EE7F2D;">Doc</a>
					  /
					  <a href="https://github.com/QianYu-Lab/PyTorch-SVGRender"; style="color: #EE7F2D;">Code</a>
					  <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/QianYu-Lab/PyTorch-SVGRender.svg" alt="GitHub stars" title="" /> 
					  <p></p>
					</td><tr> 
		
				</table></p>
		

	<p style="text-align:left"><strong><font size="4px">Work Experience</font></strong></p>
		<ul>
		    <li><p style="text-align:left"><b>AIsphere</b> (Algorithm Intern), <em>2023.05~Now</em>. </p>
				<p style="text-align:left"><b>Main job content:</b> The construction and tuning of the basic model of video generation; 
				The construction of the controllable video generation pipeline.</p></li>
		</ul><br>

	<p style="text-align:left"><strong><font size="4px">Awards</font></strong></p>
		<ul>
		    <li><p style="text-align:left">ACM-ICPC Asia Regional Contest Silver Medal, 2019</p></li>
		    <li><p style="text-align:left">Huawei Smart Dock Scholarship, 2023</p></li>
		    <li><p style="text-align:left">Outstanding graduate of Beihang University, 2023</p></li>
		    <li><p style="text-align:left">Outstanding student award(Top 5%) of Beihang University, 2020;2021;2022</p></li>
		</ul><br>

	<p style="text-align:justify"><strong><font size="4px">Professional activities</font></strong></p>
		<ul>
			<li><p style="text-align:left">Reviewer for CVPR</p></li>
	    </ur>


		<script src="script.js"></script><table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
              Design and source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
              <br>
              </p>
            </td>
          </tr>
        </tbody></table>

</body>          

</html>
