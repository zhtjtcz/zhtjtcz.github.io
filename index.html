<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haitao Zhou</title>
  
  <meta name="author" content="Haitao Zhou">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name><strong><font size="4px">Haitao Zhou (周海涛)</font></strong></name>
              </p>
              <p> I am a Master's student from Beihang University(BUAA).
				And I am also an algorithm intern of <a href="https://aishiai.com/">AIsphere</a>, 
				where I work on video generation.
              <p> I got my Undergraduate's degree from the BUAA.
              <p> Recently, my research focuses on <b>deep generative models(Diffusion models and its application) and sketch&SVG generation</b>. 
			 
              
			  <p style="text-align:center">
                <a href="mailto:marvolo@buaa.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com.hk/citations?user=e_kUVpcAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/zhtjtcz/">Github</a>
              </p>

            </td>
            
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="./photos/jiangyi.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="./photos/zht.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

		<!--         
	<p style="text-align:left"><strong><font size="4px">Research Highlight</font></strong></p> 
		<ul>
      <li><p style="text-align:left"><b>UNINEXT</b> accepted by CVPR'23. UNINEXT unifies 10 instance perception tasks using a single model with the same model parameters</p></li> 
		  
      <li><p style="text-align:left"><b>ByteTrack</b> ranks <a href="https://www.paperdigest.org/2023/01/most-influential-eccv-papers-2023-01/"; style="color: #EE7F2D;"> <b>1th of the most influential papers in ECCV 2022.</b></a> Code is available on github with 3.4k stars </p></li>
      
      <li><p style="text-align:left"><b>Spark</b> accepted by ICLR'23 as <b style="color: red">Spotlight. </b>Spark is the first successful BERT/MAE-style pretraining on any convolutional networks</p></li>
      
      <li><p style="text-align:left"><b>Unicorn</b> accepted by ECCV'22 as <b style="color: red">Oral Presentation</b> . Unicorn accomplishes the great unification of the tracking network architecture and learning paradigm</p></li>
      
      <li><p style="text-align:left"><b>IDOL</b> and <b>Seqformer</b> accepted by ECCV'22 as <b style="color: red">Oral Presentation</b> , serving as strong baseline for video instance segmentation</p></li> 
      
      <li><p style="text-align:left"><b>Sparse R-CNN</b> accepted by CVPR'21. Sparse R-CNN is integrated into several famous frameworks(Detectron2, MMDetection, PaddlePaddle)</p></li>
    	</ul> -->
	
	<br>

    
 	<p style="text-align:justify">
        <strong>
            <svg xmlns="http://www.w3.org/2000/svg" width="1.4em" height="1.4em" viewBox="0 0 64 64"><path fill="#34484c" d="M6.203 57.477s1.828 7.028 13.26 5.228L21.412 58l-3.289-3.573l-9.105-3.005l-2.812 6.055"/><path fill="#78111f" d="M42.635 44.305c0 2.49-1.282 3.146-4.512 4.511L10.515 60.471A4.514 4.514 0 0 1 6 55.959V17.166c0-2.493 1.466-3.32 4.515-4.511L38.123 1a4.513 4.513 0 0 1 4.512 4.513z"/><path fill="#34484c" d="M8.511 59.998s10.348 2.966 10.951 2.71c.604-.255-.386-4.484-.386-4.484l-3.743-4.035l-3.481-.161l-.614.871z"/><path fill="#f4f5f5" d="M37.472 3.684L6.848 16.551l1.436 8.247l4.56.454l15.214-2.447l9.458-4.959l6.962-9.535c-1.522-2.728-3.757-4.42-7.006-4.627"/><path fill="#34484c" d="M14.96 19.405c0-.72.129-1.3.373-1.792c-3.131-.292-6.09-1.199-8.25-3.146c-.715.661-1.07 1.48-1.07 2.701v38.793a4.513 4.513 0 0 0 4.513 4.512l4.454-1.882a4 4 0 0 1-.02-.391z"/><path fill="#be202e" d="M52.384 48.128c0 2.491-1.282 3.144-4.512 4.51L19.46 62.707a4.51 4.51 0 0 1-4.51-4.508V19.405c0-2.491 1.463-3.32 4.51-4.51L47.872 4.822c2.49 0 4.512 2.02 4.512 4.512z"/><path fill="#78111f" d="M40.62 16.661a.61.61 0 0 1-.35.79l-15.106 5.823a.609.609 0 1 1-.439-1.137l15.107-5.822a.607.607 0 0 1 .788.348m0 2.774a.61.61 0 0 1-.35.79l-15.106 5.822a.61.61 0 1 1-.439-1.138l15.107-5.823a.606.606 0 0 1 .788.349m-5.135 4.862c.121.313.047.634-.168.717L25 28.994c-.215.083-.488-.105-.609-.419c-.12-.313-.044-.638.17-.72l10.317-3.978c.216-.082.486.104.607.422"/></svg>
            <font size="4px">Publications</font></strong> (* equal contribution, <sup>†</sup> corresponding author)
	<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">

    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/svgdreamer_cvpr2024.png" style="height: 200px; width: 320px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.16476">
              <papertitle><b>SVGDreamer: Text Guided SVG Generation with Diffusion Model</b></papertitle>
              </a>
              <br>
              Xingming Xing, <b>Haitao Zhou</b>, Chuang Wang, Jing Zhang, Dong Xu, Qian Yu<sup>†</sup>
              <br>
              <em>Computer Vision and Pattern Recognition(<b>CVPR</b>)</em>, 2024
              <br>

			  <a href="https://arxiv.org/abs/2312.16476"; style="color: #EE7F2D;">Paper</a>
			  /
			  <a href="https://github.com/ximinng/SVGDreamer"; style="color: #EE7F2D;">Code</a>
			  <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ximinng/SVGDreamer.svg" alt="GitHub stars" title="" />
            </td><tr> 


        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/diffsketcher_nips2023.png" style="height: 160px; width: 320px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2306.14685">
              <papertitle><b>DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models</b></papertitle>
              </a>
              <br>
              Xingming Xing, Chuang Wang, <b>Haitao Zhou</b>, Jing Zhang, Qian Yu<sup>†</sup>, Dong Xu
              <br>
              <em>Neural Information Processing Systems (<b>NeurIPS</b>)</em>, 2023
              <br>

			<a href="https://arxiv.org/abs/2306.14685"; style="color: #EE7F2D;">Paper</a>
			/
            <a href="https://github.com/ximinng/DiffSketcher"; style="color: #EE7F2D;">Code</a>
            <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ximinng/DiffSketcher.svg" alt="GitHub stars" title="" />
            </td><tr>
						       
</table></p>

<p style="text-align:justify">
    <strong>
        <svg xmlns="http://www.w3.org/2000/svg" width="1.4em" height="1.4em" viewBox="0 0 48 48"><g fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="4"><path stroke="#000" d="M22 44L21 36"/><path fill="#2f88ff" stroke="#000" d="M42 44V12H26L27 20L28 28L29 36L22 44H42Z"/><path stroke="#fff" d="M28 28H33"/><path stroke="#fff" d="M27 20H33"/><path fill="#2f88ff" stroke="#000" d="M6 4H25L26 12L27 20L28 28L29 36H21H6V4Z"/><path stroke="#fff" d="M12 12H19"/><path stroke="#fff" d="M12 20H20"/><path stroke="#fff" d="M12 28H21"/></g></svg>
        <font size="4px">Preprints</font></strong> (* equal contribution, <sup>†</sup> corresponding author)
   
    <table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">
        <td style="padding:20px;width:25%;vertical-align:middle">
               <img src="./photos/trackgo.png" style="height: 180px; width: 320px; margin-top: 10px">
               <td style="padding:10px;width:75%;vertical-align:middle">
                 <a href="https://arxiv.org/abs/2408.11475">
                 <papertitle><b>TrackGo: A Flexible and Efficient Method for Controllable Video Generation</b></papertitle>
                 </a>
                 <br>
                 <b>Haitao Zhou*</b>, Chuang Wang*, Rui Nie, Jinxiao Lin, Dnogdong Yu, Qian Yu<sup>†</sup>, Changhu Wang<sup>†</sup>
                 <br>
                 <em>arXiv</em>, 2024
                 <br>
                 <a href="https://arxiv.org/abs/2408.11475"; style="color: #EE7F2D;">arXiv</a>
                 <p></p>
               </td><tr> 
           </table></p>

    <table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">
     <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/inversion_iccv2023.png" style="height: 160px; width: 320px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2308.07665">
              <papertitle><b>Inversion-by-Inversion: Exemplar-based Sketch-to-Photo Synthesis via Stochastic Differential Equations without Training</b></papertitle>
              </a>
              <br>
              Xingming Xing, Chuang Wang, <b>Haitao Zhou</b>, Zhihao Hu, Chongxuan Li, Dong Xu, Qian Yu<sup>†</sup>              
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2308.07665"; style="color: #EE7F2D;">arXiv</a>
              /
              <a href="https://github.com/ximinng/inversion-by-inversion"; style="color: #EE7F2D;">Code</a>
			  <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ximinng/inversion-by-inversion.svg" alt="GitHub stars" title="" /> 
			  <p></p>
            </td><tr> 
        </table></p>

		<p style="text-align:justify">
            <strong>
                <svg xmlns="http://www.w3.org/2000/svg" width="1.4em" height="1.4em" viewBox="0 0 72 72"><path fill="#fcea2b" d="M35.97 6.037L27.769 22.67L9.417 25.344l13.285 12.94l-3.128 18.28l16.412-8.636l16.418 8.624l-3.141-18.278l13.275-12.95l-18.354-2.66zM14.164 41.459l-9.305.577l1.617 4.697zm43.673-.48l9.304.577l-1.617 4.697zM22.072 15.592l-3.424-8.671l-3.967 2.989zm28.328.494l7.585-5.42l-3.86-3.126zM35.92 56.978l-2.484 8.985h4.967z"/><path fill="none" stroke="#000" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M35.925 6.037L27.723 22.67L9.371 25.344l13.285 12.94l-3.128 18.28l16.412-8.636l16.419 8.624l-3.142-18.278l13.276-12.95l-18.354-2.66zm.007 50.941l-2.484 8.985h4.968zm21.884-15.9l7.778 5.139l1.535-4.724zm-43.64.381l-9.305.577l1.618 4.697zm36.237-25.373l7.584-5.42l-3.86-3.126zm-28.329-.494L18.66 6.921L14.693 9.91z"/></svg>
                <font size="4px">Projects</font></strong>
			<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">
			  
			  
			 <td style="padding:20px;width:25%;vertical-align:middle">
					<img src="./photos/svgrender.png" style="height: 140px; width: 320px; margin-top: 10px">
					<td style="padding:10px;width:75%;vertical-align:middle">
					  <a href="https://arxiv.org/abs/2308.07665">
					  <papertitle><b>PyTorch-SVGRender</b></papertitle>
					  </a>
					  <br>
					  <p>
						PyTorch-SVGRender is the go-to library for differentiable rendering methods in SVG generation. 
						It supports a variety of vectorization methods, including: <b>Image-to-SVG, Text-to-SVG, Text-to-Sketch.</b> 
					  </p> 
					  <em>Based on Pytorch</em>, 2023
					  <br>
					  <a href="https://qianyu-lab.github.io/PyTorch-SVGRender-project/"; style="color: #EE7F2D;">Project Page</a>
					  /
					  <a href="https://pytorch-svgrender.readthedocs.io/en/latest/index.html"; style="color: #EE7F2D;">Doc</a>
					  /
					  <a href="https://github.com/QianYu-Lab/PyTorch-SVGRender"; style="color: #EE7F2D;">Code</a>
					  <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/QianYu-Lab/PyTorch-SVGRender.svg" alt="GitHub stars" title="" /> 
					  <p></p>
					</td><tr> 
		
				</table></p>
		

	<p style="text-align:left">
        <strong>
            <svg xmlns="http://www.w3.org/2000/svg" width="1.4em" height="1.4em" viewBox="0 0 50 50"><g fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path stroke="#306cfe" d="M33.333 8.333A2.083 2.083 0 0 0 31.25 6.25h-12.5a2.083 2.083 0 0 0-2.083 2.083v6.25h16.666zM43.75 41.667v-25a2.083 2.083 0 0 0-2.083-2.084H8.333a2.083 2.083 0 0 0-2.083 2.084v25a2.083 2.083 0 0 0 2.083 2.083h33.334a2.083 2.083 0 0 0 2.083-2.083"/><path stroke="#344054" d="M22.917 29.167H18a8.33 8.33 0 0 1-7.583-5.042l-3.792-8.646a2.08 2.08 0 0 1 1.708-.896h33.334a2.08 2.08 0 0 1 1.708.896l-3.792 8.646A8.33 8.33 0 0 1 32 29.167h-4.917"/><path stroke="#306cfe" d="M27.083 27.083h-4.166v4.167h4.166z"/></g></svg>
            <font size="4px">Work Experience</font></strong></p>
		<ul>
		    <li><p style="text-align:left"><b>AIsphere</b> (Algorithm Intern), <em>2023.05~Now</em>. </p>
				<p style="text-align:left"><b>Main job content:</b> The construction and tuning of the basic model of video generation; 
				The construction of the controllable video generation pipeline. The lead person for the Magic Brush project.</p></li>
		</ul><br>

	<p style="text-align:left">
        <strong>
            <svg xmlns="http://www.w3.org/2000/svg" width="1.4em" height="1.4em" viewBox="0 0 50 50"><g fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path stroke="#344054" d="M25 27.083a6.25 6.25 0 1 0 0-12.5a6.25 6.25 0 0 0 0 12.5"/><path stroke="#306cfe" d="M12.063 28.27L6.25 36.584l6.313 1.875l3.916 5.292l5.875-8.375m15.584-7.106l5.812 8.313l-6.312 1.875l-3.917 5.292l-5.875-8.375"/><path stroke="#306cfe" d="M40.167 20.833c0 1.584-1.396 2.917-1.855 4.334c-.458 1.416-.145 3.354-1.041 4.583s-2.813 1.5-3.938 2.417s-2.083 2.625-3.541 3.104c-1.459.479-3.105-.438-4.688-.438s-3.27.896-4.687.438c-1.417-.459-2.313-2.23-3.542-3.104c-1.23-.875-3.146-1.188-4.042-2.417c-.896-1.23-.562-3.125-1.041-4.583c-.48-1.459-1.959-2.75-1.959-4.334s1.396-2.916 1.854-4.333s.146-3.354 1.042-4.583s2.813-1.5 4.042-2.417s1.979-2.625 3.541-3.104c1.563-.48 3.105.437 4.688.437s3.27-.896 4.688-.437C31.104 6.854 32 8.625 33.333 9.5s3.146 1.187 4.042 2.417s.563 3.125 1.042 4.583s1.75 2.75 1.75 4.333"/></g></svg>
            <font size="4px">Awards</font></strong></p>
		<ul>
		    <li><p style="text-align:left">ACM-ICPC Asia Regional Contest Silver Medal, <em>2019</em></p></li>
		    <li><p style="text-align:left">Huawei Smart Dock Scholarship, <em>2023</em></p></li>
		    <li><p style="text-align:left">Outstanding graduate of Beihang University, <em>2023</em></p></li>
		    <li><p style="text-align:left">Outstanding student award(Top 5%) of Beihang University, <em>2020;2021;2022</em></p></li>
		</ul><br>

		<p style="text-align:left">
            <strong>
                <svg xmlns="http://www.w3.org/2000/svg" width="1.4em" height="1.4em" viewBox="0 0 64 64"><path fill="#93a2aa" d="M36.6 15.1c-1.7 2.6-6.1 5.3-10 7.3c.2.2.6.8.8.8c1.1-.5 2.2-.9 3.3-1.4c2.2-1 4.4-2.3 6.4-3.8c4-3.2 5.5-12.8 8.4-13c1.7 0 3.3 2.5 4.5 3.6l1.7-1.7c.1-.1-1.3-1.4-1.4-1.5c-1.7-1.7-3.7-3.5-6.4-2.5c-3.6 1.4-5.3 9.3-7.3 12.2m9.3-10s.1 0 0 0m-1.3.2q-.15 0 0 0M2.3 60.9c-.3.4-.4.8-.2 1s.7.1 1-.2L5.5 60L4 58.5z"/><g fill="#42ade2"><path d="m28.3 26.3l9.4 9.4l23.4-25l-7.8-7.8z"/><path d="m9.372 46.795l19.729-19.728l7.778 7.778L17.15 54.573z"/></g><path fill="#c7d3d8" d="m3.2 57.7l3.1 3.1l10.9-6.2l-7.8-7.8zm28-34.3l9.4 9.4l-2.9 2.9l-9.4-9.4z"/><path fill="#42ade2" d="m53.8 3.4l6.8 6.8c1.9-1.9 1.9-4.9 0-6.8s-4.9-1.9-6.8 0"/><path fill="#c7d3d8" d="m51.953 3.748l1.414-1.414l8.344 8.344l-1.414 1.414z"/></svg>
                <font size="4px">Professional Activities</font></strong></p>
		<ul>
		    <li><p style="text-align:left">Reviewer for CVPR 2024, MM 2024, AAAI 2025</p></li>
		</ul><br>

		<p style="text-align:left">
            <strong>
                <svg xmlns="http://www.w3.org/2000/svg" width="1.4em" height="1.4em" viewBox="0 0 64 64"><path fill="#c28fef" d="M58 17.4H41L32 2l-9 15.4H6L14.5 32L6 46.6h17L32 62l9-15.4h17L49.5 32zM51.9 21l-4.4 7.5l-4.4-7.5zM32 9l4.9 8.4H27zM12.1 21h8.8l-4.4 7.5zm0 22l4.4-7.5l4.4 7.5zM32 55l-4.9-8.4H37zm7-12H25l-6.4-11L25 21h14l6.4 11zm12.9 0h-8.8l4.4-7.5z"/></svg>
                <font size="4px">Interests</font></strong></p>
		<ul>
		    <li><p style="text-align:left">Reading, Coding and Harmonica. </p></li>
		    <li><p style="text-align:left">Members of the novel sinicization group of <em>A Certain Magical Index(魔法禁书目录)</em>. </p></li>
		</ul><br>

		<script src="script.js"></script><table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
              Design and source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
              <br>
              </p>
            </td>
          </tr>
        </tbody></table>

</body>

</html>
